# ğŸŒ©ï¸ KUMO: Generative Evaluation of Complex Reasoning in Large Language Models

**Official Repository** for the paper: [Generative Evaluation of Complex Reasoning in Large Language Models]()

âœ¨ **KUMO** is a novel benchmark designed to systematically evaluate the complex reasoning capabilities of Large Language Models (LLMs) through procedurally generated reasoning games. Explore the limits of LLM reasoning and track model performance on our interactive leaderboard.

ğŸ“Š Visit our [Leaderboard & Results Page]()

---

## ğŸš€ Quick Links

- ğŸ“‚ [Benchmark Dataset](#benchmark-dataset)
- ğŸ“‘ [Benchmark Format](#benchmark-format)
- âš™ï¸ [Environment Setup](#environment-setup)
- ğŸ› ï¸ [Dataset Generation](#dataset-generation)
- ğŸ“ˆ [Evaluation](#evaluation)

---

## ğŸ“‚ Benchmark Dataset

The **KUMO** benchmark introduces procedurally generated reasoning games structured around:

- ğŸ” **Truth Set ($T$)**: Possible truths.
- ğŸ¯ **Action Set ($A$)**: Available actions.
- ğŸŒŸ **Outcomes ($\mathcal{O}$)**: Action-based outcomes.
- ğŸ“š **Knowledge Book ($K$)**: Detailed guidelines linking truths, actions, and outcomes.

### Gameplay Mechanics:
- A valid truth ($t^*$) is secretly chosen.
- Players take actions and observe outcomes.
- Deduce the truth efficiently using logic and reasoning.

ğŸ§‘â€âš•ï¸ **Example Scenario**: Diagnosing diseases using medical tests.

ğŸ“Œ **Provided Domains**:
- 100 autogenerated exemplar domains
- Categories: Computer Science, Biology, Art, and more
- Typical domain: ~50 truths, ~30 actions

![KUMO Example](https://github.com/linhaowei1/kumo/blob/main/miscs/fig1.png)

---

## ğŸ“‘ Benchmark Format

The KUMO dataset is provided in **JSON format**, simplifying integration and customization. Data available at [KUMO/env](https://github.com/linhaowei1/kumo/tree/main/env):

```
kumo/
â””â”€â”€ env/
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ [DomainName]_data.py
    â”œâ”€â”€ [DomainName]/
    â”‚   â”œâ”€â”€ knowledge_book/
    â”‚   â”‚   â””â”€â”€ truth_num=4+action_num=6+valid_truth_num=1/
    â”‚   â”‚       â”œâ”€â”€ seed=0.txt
    â”‚   â”‚       â””â”€â”€ ...
    â”‚   â””â”€â”€ truth_num=4+action_num=6+valid_truth_num=1.jsonl
    â””â”€â”€ [DomainName].py
```

âš™ï¸ **Customize** parameters (`truth_num`, `action_num`, etc.) easily for tailored benchmarking.

---

## âš™ï¸ Environment Setup

### ğŸ”½ Clone the Repository

```bash
git clone https://github.com/linhaowei1/kumo.git
cd kumo
```

### ğŸ“¦ Install Dependencies

Recommended: Conda with Python (3.10 to 3.12):

```bash
conda create -n kumo python=3.12
conda activate kumo
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Dataset Generation

Create customized domains and scenarios easily:

### 1ï¸âƒ£ **Seed Configuration**

Generate scenarios via LLM:

```bash
python generate/config_generation.py \
  --load_type OPENAI \
  --api_base http://localhost:8001/v1 \
  --api_key EMPTY \
  --data_path ./templates/config_generation.jsonl
```

ğŸ”— [Detailed Instructions](https://github.com/linhaowei1/kumo/blob/main/examples/config_generation.sh)

### 2ï¸âƒ£ **Task Instances via SAT Sampling**

Generate specific tasks:

```bash
python SAT_sampling.py \
  --truth_num 4 \
  --action_num 6 \
  --valid_truth_num 1 \
  --data_num 50 \
  --domain MedicalEnv
```

ğŸ”— [Example Script](https://github.com/linhaowei1/kumo/blob/main/examples/SAT_sampling.sh)

### 3ï¸âƒ£ **Knowledge Book Generation**

Automatically build detailed knowledge bases:

```bash
python knowledge_book_generation.py \
  --load_type OPENAI \
  --api_base http://localhost:8001/v1 \
  --api_key EMPTY \
  --data_num 50 \
  --truth_num 4 \
  --action_num 6 \
  --valid_truth_num 1 \
  --domain MedicalEnv
```

ğŸ”— [Example](https://github.com/linhaowei1/kumo/blob/main/examples/knowledge_book_generation.sh)

### 4ï¸âƒ£ **Optional Knowledge Book Refinement**

Improve generated knowledge books:

```bash
python generate/knowledge_book_revision.py \
  --load_type OPENAI \
  --api_base http://localhost:8001/v1 \
  --api_key EMPTY \
  --domain MedicalEnv \
  --revision_template_path ./templates/revision_template.md
```

ğŸ”— [Revision Details](https://github.com/linhaowei1/kumo/blob/main/examples/knowledge_book_revision.sh)

---

## ğŸ“ˆ Evaluation

### ğŸ¤– **Vanilla Agent**
Evaluate standard models (e.g., GPT-4o, Anthropic APIs):

- Results: [results/](https://github.com/linhaowei1/kumo/tree/main/results)
- Example scripts: [examples/main.sh](https://github.com/linhaowei1/kumo/tree/main/examples/main.sh)

Customize by adding your own API keys and parameters.

### ğŸ¨ **Custom Agents**

Design your custom reasoning agents (e.g., ReactAgent):

ğŸ”— [Agent Templates](https://github.com/linhaowei1/kumo/tree/main/agents)

---

ğŸ’¬ **Support & Questions**

For support, feedback, or inquiries, please:
- Open an issue on GitHub
- Contact the repository maintainers directly
