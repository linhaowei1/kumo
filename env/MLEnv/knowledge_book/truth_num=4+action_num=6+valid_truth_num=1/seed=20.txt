# Machine Learning Model Analysis Guidebook

This guidebook presents an overview of selected machine learning models and evaluation techniques. It explains how different outcomes of evaluation techniques can lead to the exclusion of certain models based on their observed performance. This approach is designed to help practitioners eliminate unsuitable models by interpreting test results.

## Machine Learning Models

1. **XGBoost (Extreme Gradient Boosting)**
   - A powerful machine learning algorithm based on gradient boosting decision trees. It is known for its effectiveness in both classification and regression tasks.
   - Features parallel processing, regularization parameters to prevent overfitting, and the ability to handle missing values.

2. **K-Nearest Neighbors (K-NN)**
   - An instance-based learning algorithm where classification decisions are made based on the majority vote of an object's neighbors.
   - It is intuitive and simple, yet can become computationally expensive with large datasets.

3. **Random Guess Model**
   - A benchmark model that makes predictions by randomly guessing the class without considering input features.
   - Useful as a baseline comparison to demonstrate how well other models perform.

4. **Gradient Boosting Machine (GBM)**
   - Similar to XGBoost, GBM builds models by combining the predictions from multiple decision trees to improve accuracy.
   - Though powerful, it can be sensitive to overfitting without proper tuning.

## Evaluation Techniques

### 1. Test for Convergence

This test checks if the model successfully converges during training. Convergence reflects the model's ability to learn from the data.

- **Did Not Converge**: If a model does not converge, it may indicate problems with hyperparameters or data quality. In such cases, eliminate:
  - **XGBoost**
  - **Gradient Boosting Machine**

### 2. Evaluate Feature Importances

This technique assesses which features contribute most to the modelâ€™s predictions.

- **Feature Importance Not Available**: For models where feature importance is not applicable, disregard:
  - **K-Nearest Neighbors**

### 3. Calculate AUC (Area Under the ROC Curve)

AUC is a performance metric for classification models, indicating how well the model separates classes.

- **AUC between 0.7 and 0.9**: Models with moderate AUC values, rule out:
  - **K-Nearest Neighbors**
- **Typically lower or higher than other ranges, no rule out applies**

### 4. Compute Recall

Recall measures the ability of a model to find all relevant instances from the data.

- **Recall between 0.5 and 0.8**: Models offering this level of recall might be lacking in distinguishing finer nuances:
  - **K-Nearest Neighbors**
- **Other ranges indicate no immediate exclusions**

### 5. Evaluate Classification Accuracy

This metric shows how often the model makes correct predictions.

- **Accuracy between 0.0 and 0.6**: Models with low accuracy are less reliable and should be excluded:
  - **K-Nearest Neighbors**

### 6. Compute F1 Score

The F1 Score is the harmonic mean of precision and recall, offering a balance between the two.

- **F1 Score between 0.0 and 0.6**: Models with this range may not maintain a good balance and should be ruled out:
  - **K-Nearest Neighbors**

---

By following this structured approach, practitioners can effectively rule out models that do not meet the desired criteria during each evaluation. The outlined exclusions help simplify the selection process, focusing attention on the remaining viable models.