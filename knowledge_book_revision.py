import json
import os

from transformers import HfArgumentParser
from tqdm import tqdm

from common.configs import (
    Data_Arguments,
    Knowledge_Book_Revision_Arguments,
    LLM_Arguments,
)
from common.utils import get_env, get_llm


def get_evaluate_prompt(knowledge_book_prompt, existing_knowledge_book, ta_mapping,template):
    parts = [p for p in knowledge_book_prompt.split('\n') if p]
    
    input_section = f"## **Input**\n{parts[1]}\n{parts[2]}\n{parts[3].replace('Outcomes', 'TA mapping')}\n"
    
    start = next(i for i, p in enumerate(parts) if '1.' in p)
    end = next(i for i, p in enumerate(parts) if '2.' in p)
    #input_description = f"## **Input description**\n{'\n'.join(parts[start:end]).replace('Outcomes', 'TA mapping').replace('1. ', '')}\n"
    formatted_text = '\n'.join(parts[start:end]).replace('Outcomes', 'TA mapping').replace('1. ', '')
    input_description = f"## **Input description**\n{formatted_text}\n"

    
    evaluate_prompt = (
        "Please evaluate an existing knowledge book generated by LLM based on the following information:\n"
        f"{input_section}"
        f"{input_description}"
        "Existing Knowledge Book is the knowledge book to be evaluated.\n"
        f"{template}\n"
        f"Please evaluate the existing knowledge book based on the above criteria: {existing_knowledge_book}, "
        f"Note again TA_mapping: {ta_mapping}. Please analyze **every** TA_mapping and its corresponding part in the "
        "existing knowledge book based on the evaluation criteria and explain whether it satisfies the criteria. "
        "Illustrate your reasons and if it is accurate, please output `<ANSWER>True</ANSWER>`, else output `<ANSWER>False</ANSWER>`."
        f"If the final answer is **False**, please revise the existing knowledge book and wrap the complete revised knowledge book in <BOOK> ${{revised knowledge book}} </BOOK>. "
        "**Note**: Please maintain the original language style, only correct the errors in the existing knowledge book when generating the revised knowledge book."
    )
    
    return evaluate_prompt

    
def main():
    llm_args, revision_args, data_args = HfArgumentParser((LLM_Arguments, Knowledge_Book_Revision_Arguments, Data_Arguments)).parse_args_into_dataclasses()
    
    # Get env
    domain = data_args.domain
    env = get_env(domain)
    
    # Get LLM
    llm = get_llm(llm_args)
    
    # Get template
    template_path = revision_args.revision_template_path
    with open(template_path, 'r', encoding="utf-8") as f:
        template = f.read()
    
    # Get dataset
    ds_path = data_args.dataset_path
    data_num = data_args.data_num
    kb_dir = data_args.knowledge_book_dir
    with open(ds_path, "r", encoding="utf-8") as f:
        ds = [json.loads(line) for line in f]
        ds = ds[:data_num]
    modified_seeds = []
    
    # Eval
    for data in tqdm(ds):
        # read original knowledge book
        seed = data["seed"]
    
        kb_path = os.path.join(kb_dir, f"seed={seed}.txt")
        with open(kb_path, "r", encoding="utf-8") as f:
            existing_kb = f.read()
        
        # reconstruct ta_mapping information based on the specific case
        sub_ta_mapping = {action: env.ta_mapping[action]['states'] for action in data['actions']}
        for k, v in sub_ta_mapping.items():
            new_v = {
                state: {value for value in v[state] if value in data['truths']}
                for state in v
            }
            sub_ta_mapping[k] = new_v
            
        # get knowledge book (generation) prompt to fit the template
        kb_prompt = env.get_knowledge_book_prompt(data['truths'], data['actions'], sub_ta_mapping)
        
        # get prompt for using LLM to evaluate kb
        evaluate_prompt = get_evaluate_prompt(kb_prompt, existing_kb, sub_ta_mapping, template)
        messages = [
            {"role":"user", "content":evaluate_prompt}
        ]
        
        # get results
        evaluate_outcome = llm.call_llm(messages)
        answer = evaluate_outcome.split("<ANSWER>")[-1].split("</ANSWER>")[0].strip()

        if 'true' in answer.lower():
            continue
        else:
            new_book = evaluate_outcome.split('<BOOK>')[-1].split('</BOOK>')[0]
            if len(new_book) > 0:
                output_path = os.path.join(kb_dir, f'seed={seed}.txt')
                with open(output_path, 'w') as f:
                    f.write(new_book)
                    modified_seeds.append(seed)
            else:
                print(f.write(f'Evaluate Error: domain: {domain}; seed: {seed}'))
        
    print(f"domain:{domain} - revised seeds: {modified_seeds}")

if __name__ == "__main__":
    main()